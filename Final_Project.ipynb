{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ecda751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import tiktoken\n",
    "from docx import Document\n",
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28cb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_law_header(text):\n",
    "    \"\"\"\n",
    "    T√¨m \"Ch∆∞∆°ng I\" v√† l·∫•y t·ª´ ƒë√≥ tr·ªü ƒëi (ƒêi·ªÅu 1 n·∫±m TRONG Ch∆∞∆°ng I).\n",
    "    \"\"\"\n",
    "    pattern = r\"(Ch∆∞∆°ng\\s+[I1])\"\n",
    "    match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        # L·∫•y t·ª´ \"Ch∆∞∆°ng I\" tr·ªü ƒëi (ƒêi·ªÅu 1 n·∫±m sau ƒë√≥)\n",
    "        return text[match.start():]\n",
    "    else:\n",
    "        # Kh√¥ng t√¨m th·∫•y Ch∆∞∆°ng I => gi·ªØ nguy√™n\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b084c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def clean_text(t):\n",
    "    \"\"\"Gi·ªØ l·∫°i newline ƒë·ªÉ regex t√°ch ƒëi·ªÅu\"\"\"\n",
    "    t = t.replace(\"\\xa0\", \" \")\n",
    "    t = re.sub(r\"[ \\t]+\", \" \", t)  \n",
    "    return t.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c04f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#T√°ch ƒëi·ªÅu-kho·∫£n\n",
    "\n",
    "regex_dieu = re.compile(r\"(?m)^(ƒêi·ªÅu\\s+\\d+)\\.\")  \n",
    "regex_khoan = re.compile(r\"(?<=\\n|\\s)(\\d{1,2})\\.(?=\\s)\")   \n",
    "regex_diem = re.compile(r\"^[a-z]\\)$\")  \n",
    "\n",
    "def split_by_dieu(text):\n",
    "    parts = regex_dieu.split(text)\n",
    "    results = []\n",
    "    \n",
    "    for i in range(1,len(parts), 2):\n",
    "        if i + 1 < len(parts):\n",
    "            dieu_title = parts[i].strip()\n",
    "            dieu_content = parts[i+1].strip()\n",
    "            results.append((dieu_title, dieu_content))\n",
    "\n",
    "    return results\n",
    "\n",
    "def split_by_khoan(dieu_content):\n",
    "    parts = regex_khoan.split(dieu_content)\n",
    "    results = []\n",
    "\n",
    "    for i in range(1, len(parts), 2):\n",
    "        khoan_num = parts[i]\n",
    "        khoan_content = parts[i+1].strip()\n",
    "        results.append((khoan_num, khoan_content))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def split_by_diem(khoan_content):\n",
    "    parts = regex_diem.split(khoan_content)\n",
    "    results = []\n",
    "\n",
    "    for i in range(1, len(parts), 2):\n",
    "        diem_letter = parts[i]\n",
    "        diem_content = parts[i+1].strip()\n",
    "        results.append((diem_letter, diem_content))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43229c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ƒêo token\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7cf3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(law_name, text):\n",
    "    chunks = []\n",
    "    ds_dieu = split_by_dieu(text)\n",
    "\n",
    "    for dieu_title, dieu_body in ds_dieu:\n",
    "        nums = re.findall(r\"\\d+\", dieu_title)\n",
    "        if not nums:\n",
    "            continue\n",
    "        dieu_num = int(nums[0])  \n",
    "\n",
    "        ds_khoan = split_by_khoan(dieu_body)\n",
    "\n",
    "        if not ds_khoan or len(ds_khoan) == 0:\n",
    "            chunk_id = f\"{law_name}_d{dieu_num}\"\n",
    "            chunks.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"law\": law_name,\n",
    "                \"dieu\": dieu_num, \n",
    "                \"khoan\": None,\n",
    "                \"diem\": None,\n",
    "                \"text\": f\"[ƒêi·ªÅu {dieu_num}]: {dieu_body.strip()}\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        for khoan_num, khoan_body in ds_khoan:\n",
    "            chunk_id = f\"{law_name}_d{dieu_num}_k{khoan_num}\"\n",
    "            \n",
    "            chunks.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"law\": law_name,\n",
    "                \"dieu\": dieu_num,  \n",
    "                \"khoan\": int(khoan_num),\n",
    "                \"diem\": None,\n",
    "                \"text\": f\"[ƒêi·ªÅu {dieu_num}] [Kho·∫£n {khoan_num}]: {khoan_body.strip()}\"\n",
    "            })\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d2ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T·ªïng chunk t·∫°o ƒë∆∞·ª£c: 338\n",
      "ƒê√£ l∆∞u file luatgtdb_chunks.json ‚úî\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    PDF_PATH = \"luatgtdb.pdf\"        \n",
    "    LAW_NAME = \"168/2024/Nƒê-CP\"\n",
    "\n",
    "    raw_text = extract_text(PDF_PATH)\n",
    "    clean = clean_text(raw_text)\n",
    "\n",
    "    chunks = create_chunks(LAW_NAME, clean)\n",
    "\n",
    "    print(\"T·ªïng chunk t·∫°o ƒë∆∞·ª£c:\", len(chunks))\n",
    "\n",
    "    with open(\"luatgtdb_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"ƒê√£ l∆∞u file luatgtdb_chunks.json ‚úî\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32739224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ngona\\anaconda3\\envs\\trash-backend\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64a37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter\n",
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  \n",
    "from langchain_community.embeddings import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bbf794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫§U H√åNH API KEY GEMINI\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDjjNTAujGgYUPgLk644qIyV6xxgGSwwYc\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d431469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "class STEmbedder(Embeddings):\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        texts = [f\"passage: {t}\" for t in texts]\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        text = f\"query: {text}\"\n",
    "        return self.model.encode([text], convert_to_numpy=True)[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e46b6b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KH·ªûI T·∫†O LLM & EMBEDDING\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "embeddings = STEmbedder(\"intfloat/multilingual-e5-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c92950ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ load 338 chunks t·ª´ JSON.\n"
     ]
    }
   ],
   "source": [
    "# LOAD JSON & T·∫†O DOCUMENTS\n",
    "JSON_PATH = \"luatgtdb_chunks.json\"   \n",
    "\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_chunks = json.load(f)   # list[dict]\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=item[\"text\"],\n",
    "        metadata={\n",
    "            \"id\": item.get(\"id\"),\n",
    "            \"law\": item.get(\"law\"),\n",
    "            \"dieu\": item.get(\"dieu\"),\n",
    "            \"khoan\": item.get(\"khoan\"),\n",
    "            \"diem\": item.get(\"diem\"),\n",
    "        },\n",
    "    )\n",
    "    for item in raw_chunks\n",
    "]\n",
    "\n",
    "print(f\"ƒê√£ load {len(documents)} chunks t·ª´ JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d86a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore ƒë√£ kh·ªüi t·∫°o xong.\n"
     ]
    }
   ],
   "source": [
    "# T·∫†O VECTORSTORE & RETRIEVER\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 30})\n",
    "print(\"Vectorstore ƒë√£ kh·ªüi t·∫°o xong.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebb59d",
   "metadata": {},
   "source": [
    "## So s√°nh c√°c Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69de502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. VECTOR SEARCH RETRIEVER (Hi·ªán t·∫°i)\n",
    "\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 30})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0f05c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BM25 RETRIEVER (Keyword-based)\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8385bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. HYBRID RETRIEVER (K·∫øt h·ª£p Vector + BM25)\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    \"\"\"Hybrid retriever k·∫øt h·ª£p Vector Search + BM25\"\"\"\n",
    "    vector_retriever: BaseRetriever\n",
    "    bm25_retriever: BaseRetriever\n",
    "    \n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        vector_docs = self.vector_retriever.invoke(query)\n",
    "        bm25_docs = self.bm25_retriever.invoke(query)\n",
    "        \n",
    "        # K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ c·∫£ hai retriever\n",
    "        # Lo·∫°i b·ªè tr√πng l·∫∑p d·ª±a tr√™n metadata['id']\n",
    "        doc_dict = {}\n",
    "        for doc in vector_docs:\n",
    "            doc_id = doc.metadata.get(\"id\")\n",
    "            doc_dict[doc_id] = doc\n",
    "        for doc in bm25_docs:\n",
    "            doc_id = doc.metadata.get(\"id\")\n",
    "            if doc_id not in doc_dict:\n",
    "                doc_dict[doc_id] = doc\n",
    "        \n",
    "        return list(doc_dict.values())\n",
    "    \n",
    "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        return self._get_relevant_documents(query)\n",
    "\n",
    "# Kh·ªüi t·∫°o Hybrid Retriever\n",
    "hybrid_retriever = HybridRetriever(\n",
    "    vector_retriever=vector_retriever,\n",
    "    bm25_retriever=bm25_retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc0d7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load eval dataset\n",
    "eval_file = \"evaluation.json\"\n",
    "with open(eval_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "\n",
    "# H√†m ki·ªÉm tra n·∫øu retrieved documents ch·ª©a gold document\n",
    "def check_if_relevant(retrieved_docs, gold_id):\n",
    "    \"\"\"Ki·ªÉm tra xem gold_id c√≥ trong retrieved documents kh√¥ng\"\"\"\n",
    "    for doc in retrieved_docs:\n",
    "        if doc.metadata.get(\"id\") == gold_id:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# H√†m t√≠nh rank c·ªßa gold document (position n√≥ xu·∫•t hi·ªán)\n",
    "def get_rank(retrieved_docs, gold_id):\n",
    "    \"\"\"L·∫•y v·ªã tr√≠ (rank) c·ªßa gold document trong retrieved list\"\"\"\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        if doc.metadata.get(\"id\") == gold_id:\n",
    "            return i + 1  # 1-indexed\n",
    "    return None\n",
    "\n",
    "\n",
    "total_queries = len(eval_data)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d39a9fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Retriever Hit Rate (%)    MRR Precision@5 (%) Recall@5 (%) NDCG@5 Precision@10 (%) Recall@10 (%) NDCG@10 Mean Rank\n",
      "VECTOR SEARCH        96.4% 0.7610           86.2%        86.2% 0.7782            92.4%         92.4%  0.7982       2.4\n",
      "         BM25        87.4% 0.7727           87.4%        87.4% 0.7982            87.4%         87.4%  0.7982       1.3\n",
      "       HYBRID        98.0% 0.7615           86.2%        86.2% 0.7782            92.4%         92.4%  0.7982       2.9\n"
     ]
    }
   ],
   "source": [
    "# 6b. T√çNH TH√äM METRIC: PRECISION, NDCG, HIT RATE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_ndcg(rank, k):\n",
    "    \"\"\"T√≠nh NDCG (Normalized Discounted Cumulative Gain)\"\"\"\n",
    "    if rank is None or rank > k:\n",
    "        return 0\n",
    "    # DCG: 1 / log2(rank+1)\n",
    "    dcg = 1.0 / np.log2(rank + 1)\n",
    "    # Ideal DCG: lu√¥n l√† 1 (best rank l√† 1)\n",
    "    idcg = 1.0\n",
    "    return dcg / idcg\n",
    "\n",
    "# Recalculate v·ªõi th√™m metrics\n",
    "results_extended = {\n",
    "    \"vector_search\": {\n",
    "        \"hit\": 0, \"mrr\": 0, \"recall_5\": 0, \"recall_10\": 0,\n",
    "        \"precision_5\": 0, \"precision_10\": 0,\n",
    "        \"ndcg_5\": 0, \"ndcg_10\": 0,\n",
    "        \"ranks\": []\n",
    "    },\n",
    "    \"bm25\": {\n",
    "        \"hit\": 0, \"mrr\": 0, \"recall_5\": 0, \"recall_10\": 0,\n",
    "        \"precision_5\": 0, \"precision_10\": 0,\n",
    "        \"ndcg_5\": 0, \"ndcg_10\": 0,\n",
    "        \"ranks\": []\n",
    "    },\n",
    "    \"hybrid\": {\n",
    "        \"hit\": 0, \"mrr\": 0, \"recall_5\": 0, \"recall_10\": 0,\n",
    "        \"precision_5\": 0, \"precision_10\": 0,\n",
    "        \"ndcg_5\": 0, \"ndcg_10\": 0,\n",
    "        \"ranks\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "for idx, item in enumerate(eval_data):\n",
    "    query = item[\"query\"]\n",
    "    gold_id = item[\"gold_id\"]\n",
    "    \n",
    "    # Vector Search\n",
    "    vs_docs = vector_retriever.invoke(query)\n",
    "    vs_rank = get_rank(vs_docs, gold_id)\n",
    "    \n",
    "    # BM25\n",
    "    bm25_docs = bm25_retriever.invoke(query)\n",
    "    bm25_rank = get_rank(bm25_docs, gold_id)\n",
    "    \n",
    "    # Hybrid\n",
    "    hybrid_docs = hybrid_retriever.invoke(query)\n",
    "    hybrid_rank = get_rank(hybrid_docs, gold_id)\n",
    "    \n",
    "    for retriever_name, rank in [(\"vector_search\", vs_rank), (\"bm25\", bm25_rank), (\"hybrid\", hybrid_rank)]:\n",
    "        if rank is not None:\n",
    "            results_extended[retriever_name][\"hit\"] += 1\n",
    "            results_extended[retriever_name][\"mrr\"] += 1 / rank\n",
    "            results_extended[retriever_name][\"ranks\"].append(rank)\n",
    "            \n",
    "            # Precision@5 v√† @10\n",
    "            if rank <= 5:\n",
    "                results_extended[retriever_name][\"precision_5\"] += 1\n",
    "                results_extended[retriever_name][\"recall_5\"] += 1\n",
    "            if rank <= 10:\n",
    "                results_extended[retriever_name][\"precision_10\"] += 1\n",
    "                results_extended[retriever_name][\"recall_10\"] += 1\n",
    "            \n",
    "            # NDCG@5 v√† @10\n",
    "            results_extended[retriever_name][\"ndcg_5\"] += calculate_ndcg(rank, 5)\n",
    "            results_extended[retriever_name][\"ndcg_10\"] += calculate_ndcg(rank, 10)\n",
    "\n",
    "\n",
    "metrics_comprehensive = []\n",
    "\n",
    "for retriever_name in [\"vector_search\", \"bm25\", \"hybrid\"]:\n",
    "    data = results_extended[retriever_name]\n",
    "    \n",
    "    hit_rate = (data[\"hit\"] / total_queries * 100) if total_queries > 0 else 0\n",
    "    mrr = (data[\"mrr\"] / total_queries) if total_queries > 0 else 0\n",
    "    precision_5 = (data[\"precision_5\"] / total_queries * 100) if total_queries > 0 else 0\n",
    "    precision_10 = (data[\"precision_10\"] / total_queries * 100) if total_queries > 0 else 0\n",
    "    recall_5 = (data[\"recall_5\"] / total_queries * 100) if total_queries > 0 else 0\n",
    "    recall_10 = (data[\"recall_10\"] / total_queries * 100) if total_queries > 0 else 0\n",
    "    ndcg_5 = (data[\"ndcg_5\"] / total_queries) if total_queries > 0 else 0\n",
    "    ndcg_10 = (data[\"ndcg_10\"] / total_queries) if total_queries > 0 else 0\n",
    "    mean_rank = (np.mean(data[\"ranks\"]) if data[\"ranks\"] else float('inf'))\n",
    "    \n",
    "    metrics_comprehensive.append({\n",
    "        \"Retriever\": retriever_name.upper().replace(\"_\", \" \"),\n",
    "        \"Hit Rate (%)\": f\"{hit_rate:.1f}%\",\n",
    "        \"MRR\": f\"{mrr:.4f}\",\n",
    "        \"Precision@5 (%)\": f\"{precision_5:.1f}%\",\n",
    "        \"Recall@5 (%)\": f\"{recall_5:.1f}%\",\n",
    "        \"NDCG@5\": f\"{ndcg_5:.4f}\",\n",
    "        \"Precision@10 (%)\": f\"{precision_10:.1f}%\",\n",
    "        \"Recall@10 (%)\": f\"{recall_10:.1f}%\",\n",
    "        \"NDCG@10\": f\"{ndcg_10:.4f}\",\n",
    "        \"Mean Rank\": f\"{mean_rank:.1f}\" if mean_rank != float('inf') else \"N/A\"\n",
    "    })\n",
    "\n",
    "df_comprehensive = pd.DataFrame(metrics_comprehensive)\n",
    "print(\"\\n\")\n",
    "print(df_comprehensive.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538e347",
   "metadata": {},
   "source": [
    "## Khuy·∫øn ngh·ªã s·ª≠ d·ª•ng\n",
    "\n",
    "### File hi·ªán t·∫°i ƒëang d√πng: **Vector Search**\n",
    "- T·ªët cho chatbot h·ªèi ƒë√°p ph√°p lu·∫≠t v√¨ c·∫ßn hi·ªÉu ng·ªØ c·∫£nh v√† √Ω nghƒ©a c·ªßa c√¢u h·ªèi\n",
    "- Tuy ch·∫≠y h∆°n BM25 nh∆∞ng k·∫øt qu·∫£ ch√≠nh x√°c h∆°n\n",
    "\n",
    "### ƒê·ªÉ thay ƒë·ªïi retriever cho RAG chain:\n",
    "1. **D√πng BM25**: Thay `vector_retriever` b·∫±ng `bm25_retriever` ·ªü cell \"X√ÇY D·ª∞NG RAG CHAIN\"\n",
    "2. **D√πng Hybrid**: Thay `vector_retriever` b·∫±ng `hybrid_retriever` (khuy·∫øn ngh·ªã nh·∫•t)\n",
    "3. **D√πng Vector Search**: Gi·ªØ nguy√™n (m·∫∑c ƒë·ªãnh hi·ªán t·∫°i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d658a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªäNH NGHƒ®A H√ÄM FORMAT CONTEXT\n",
    "\n",
    "def format_docs(docs):\n",
    "    parts = []\n",
    "    for d in docs:\n",
    "        law = d.metadata.get(\"law\")\n",
    "        dieu = d.metadata.get(\"dieu\")\n",
    "        khoan = d.metadata.get(\"khoan\")\n",
    "        diem = d.metadata.get(\"diem\")\n",
    "        header = f\"[Lu·∫≠t {law} - ƒêi·ªÅu {dieu}, Kho·∫£n {khoan}\"\n",
    "        if diem:\n",
    "            header += f\", ƒêi·ªÉm {diem}]\"\n",
    "        else:\n",
    "            header += \"]\"\n",
    "        parts.append(f\"{header}\\n{d.page_content}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d11fcb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT CHO RAG\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω, tr·∫£ l·ªùi d·ª±a tr√™n c√°c ƒëi·ªÅu kho·∫£n lu·∫≠t trong ph·∫ßn context. \"\n",
    "            \"N·∫øu c√≥ th·ªÉ, h√£y n√™u r√µ ƒêi·ªÅu/Kho·∫£n/ƒêi·ªÉm.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(\"chat_history\"),  # l·ªãch s·ª≠ h·ªôi tho·∫°i nhi·ªÅu turn\n",
    "        (\n",
    "            \"human\",\n",
    "            \"C√¢u h·ªèi: {question}\\n\\n\"\n",
    "            \"VƒÉn b·∫£n lu·∫≠t li√™n quan:\\n{context}\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a978d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X√ÇY D·ª∞NG RAG CHAIN\n",
    "\n",
    "# Chain: question -> retriever -> format_docs -> prompt -> llm\n",
    "\n",
    "base_rag_chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "        \"context\": itemgetter(\"question\")\n",
    "        | RunnableLambda(lambda q: retriever.invoke(q))\n",
    "        | RunnableLambda(format_docs),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0287d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "_store = {} \n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in _store:\n",
    "        _store[session_id] = InMemoryChatMessageHistory()\n",
    "    return _store[session_id]\n",
    "\n",
    "\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    base_rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",      \n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ea8ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    \"\"\"\n",
    "    message: c√¢u m·ªõi user g·ª≠i\n",
    "    history: l·ªãch s·ª≠ h·ªôi tho·∫°i c·ªßa Gradio (kh√¥ng c·∫ßn t·ª± x·ª≠ l√Ω, v√¨ ta d√πng InMemoryChatMessageHistory ri√™ng)\n",
    "    \"\"\"\n",
    "    # D√πng 1 session_id c·ªë ƒë·ªãnh cho Gradio. N·∫øu mu·ªën multi-user th√¨ map theo user id/cookie.\n",
    "    session_id = \"gradio-session\"\n",
    "\n",
    "    result = rag_with_history.invoke(\n",
    "        {\"question\": message},\n",
    "        config={\"configurable\": {\"session_id\": session_id}},\n",
    "    )\n",
    "\n",
    "    # ChatGoogleGenerativeAI tr·∫£ v·ªÅ AIMessage -> d√πng .content\n",
    "    return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e37efec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngona\\AppData\\Local\\Temp\\ipykernel_19480\\1456680996.py:11: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: css. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(css=custom_css) as view:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_css = \"\"\"\n",
    ":root { --radius: 14px; }\n",
    "body { background: linear-gradient(135deg, #f6f9ff, #eef2ff); }\n",
    ".gradio-container {max-width: none !important; width: min(1400px, calc(100% - 48px)); margin: 32px auto; font-family: 'Inter', system-ui, -apple-system, sans-serif;}\n",
    "#chatbot {height: 700px !important; border-radius: var(--radius); border: 1px solid #e5e7eb; box-shadow: 0 14px 36px rgba(0,0,0,0.08);}\n",
    ".gr-chatbot-message { border-radius: 12px; font-size: 15px; line-height: 1.5; }\n",
    ".gr-text-input input, .gr-text-input textarea { font-size: 15px; }\n",
    "footer {display: none !important;}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=custom_css) as view:\n",
    "    gr.ChatInterface(\n",
    "        fn=chat,\n",
    "        title=\"üö¶ CHATBOT H·ªñ TR·ª¢ H·ªéI ƒê√ÅP GIAO TH√îNG\",\n",
    "        description=\"H·ªèi ƒë√°p nhanh v·ªÅ quy ƒë·ªãnh giao th√¥ng, tr√≠ch d·∫´n ƒêi·ªÅu/Kho·∫£n/ƒêi·ªÉm li√™n quan.\",\n",
    "        chatbot=gr.Chatbot(\n",
    "            elem_id=\"chatbot\",\n",
    "            height=700,\n",
    "            avatar_images=(\n",
    "                \"https://img.icons8.com/color/96/traffic-light.png\",\n",
    "                \"https://img.icons8.com/fluency/96/police-badge.png\"\n",
    "            ),\n",
    "        ),\n",
    "        textbox=gr.Textbox(\n",
    "            placeholder=\"Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (vd: M·ª©c ph·∫°t v∆∞·ª£t ƒë√®n ƒë·ªè?)\",\n",
    "            autofocus=True,\n",
    "            lines=2,\n",
    "            submit_btn=\"G·ª≠i\",\n",
    "        ),\n",
    "        examples=[\n",
    "            [\"M·ª©c ph·∫°t khi kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm?\"],\n",
    "            [\"Gi·ªõi h·∫°n n·ªìng ƒë·ªô c·ªìn v·ªõi ng∆∞·ªùi ƒëi·ªÅu khi·ªÉn xe m√°y?\"],\n",
    "            [\"Quy ƒë·ªãnh v·ªÅ t·ªëc ƒë·ªô t·ªëi ƒëa trong khu v·ª±c ƒë√¥ng d√¢n c∆∞?\"],\n",
    "        ],\n",
    "        submit_btn=\"G·ª≠i\",\n",
    "    )\n",
    "\n",
    "view.launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trash-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
